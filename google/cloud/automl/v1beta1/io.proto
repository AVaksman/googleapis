// Copyright 2018 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

syntax = "proto3";

package google.cloud.automl.v1beta1;

import "google/api/annotations.proto";

option go_package = "google.golang.org/genproto/googleapis/cloud/automl/v1beta1;automl";
option java_multiple_files = true;
option java_package = "com.google.cloud.automl.v1beta1";
option php_namespace = "Google\\Cloud\\AutoMl\\V1beta1";


// Input configuration for ImportData Action.
//
// The format of input depends on dataset_metadata the Dataset into which
// the import is happening has. As input source the
// [gcs_source][google.cloud.automl.v1beta1.InputConfig.gcs_source]
// is expected, unless specified otherwise. If a file with identical content
// (even if it had different GCS_FILE_PATH) is mentioned multiple times , then
// its label, bounding boxes etc. are appended. The same file should be always
// provided with the same ML_USE and GCS_FILE_PATH, if it is not then
// these values are nondeterministically selected from the given ones.
//
// The formats are represented in EBNF with commas being literal and with
// non-terminal symbols defined near the end of this comment. The formats are:
//
//  *  For Image Object Detection:
//         CSV file(s) with each line in format:
//           ML_USE,GCS_FILE_PATH,LABEL,BOUNDING_BOX
//           GCS_FILE_PATH leads to image of up to 30MB in size. Supported
//           extensions: .JPEG, .GIF, .PNG.
//           Each image is assumed to be exhaustively labeled. The
//           minimum allowed BOUNDING_BOX edge length is 0.01, and no more than
//           500 BOUNDING_BOX-es per image are allowed.
//         Three sample rows:
//           TRAIN,gs://folder/image1.png,car,0.1,0.1,,,0.3,0.3,,
//           TRAIN,gs://folder/image1.png,bike,.7,.6,,,.8,.9,,
//           TEST,gs://folder/im2.png,car,0.1,0.1,0.2,0.1,0.2,0.3,0.1,0.3
//
//  *  For Video Classification:
//         CSV file(s) with each line in format:
//           ML_USE,GCS_FILE_PATH
//           where ML_USE VALIDATE value should not be used (service picks
//           validation examples from the TRAIN set). The GCS_FILE_PATH should
//           lead to another .csv file which describes examples that have
//           given ML_USE, using the following row format:
//           GCS_FILE_PATH,LABEL,TIME_SEGMENT_START,TIME_SEGMENT_END
//           Here GCS_FILE_PATH leads to a video of up to 1.5GB in size and up
//           to 1h duration. Supported extensions: .MOV, .MPEG4, .MP4, .AVI.
//           TIME_SEGMENT_START and TIME_SEGMENT_END must be within the
//           length of the video, and end has to be after the start. Any segment
//           of a video which has one or more labels on it, is considered a
//           hard negative for all other labels. Any segment with no labels on
//           it is considered to be unknown.
//         Sample top level CSV file:
//           TRAIN,gs://folder/train_videos.csv
//           TEST,gs://folder/test_videos.csv
//           UNASSIGNED,gs://folder/other_videos.csv
//         Three sample rows of a CSV file for a particular ML_USE:
//           gs://folder/video1.avi,car,120,180.000021
//           gs://folder/video1.avi,bike,150,180.000021
//           gs://folder/vid2.avi,car,0,60.5
//
//  *  For Text Extraction:
//         CSV file(s) with each line in format:
//           ML_USE,GCS_FILE_PATH
//           GCS_FILE_PATH leads to a .JSONL (i.e. JSON Lines) file.
//           A .JSONL file contains, per line, a proto that wraps a TextSnippet
//           proto (in json representation) followed by one or more
//           AnnotationPayload protos (called annotations), which have
//           display_name and text_extraction detail populated.
//           Given text is expected to be annotated exhaustively, e.g. if you
//           look for animals and text contains "dolphin" that is not labeled,
//           then "dolphin" will be assumed to not be an animal.
//           Any given text snippet content must have 30,000 characters or less,
//           and also be UTF-8 NFC encoded (ASCII already is). Any given
//           .JSONL file must be 100MB or smaller.
//         Three sample CSV rows:
//           TRAIN,gs://folder/file1.jsonl
//           VALIDATE,gs://folder/file2.jsonl
//           TEST,gs://folder/file3.jsonl
//         Sample JSON Lines file (presented here with artificial line breaks,
//         but the only actual line break is denoted by \n).:
//           {
//             "text_snippet": {
//               "content" : "dog car cat"
//             },
//             "annotations": [
//                {
//                  "display_name": "animal",
//                  "text_extraction": {
//                    "text_segment": {"start_offset": 0, "end_offset": 2}
//                  }
//                },
//                {
//                  "display_name": "vehicle",
//                  "text_extraction": {
//                    "text_segment": {"start_offset": 4, "end_offset": 6}
//                  }
//                },
//                {
//                  "display_name": "animal",
//                  "text_extraction": {
//                    "text_segment": {"start_offset": 8, "end_offset": 10}
//                  }
//                }
//             ]
//           }\n
//           {
//              "text_snippet": {
//                "content" : "This dog is good."
//              },
//              "annotations": [
//                 {
//                   "display_name": "animal",
//                   "text_extraction": {
//                     "text_segment": {"start_offset": 5, "end_offset": 7}
//                   }
//                 }
//              ]
//           }
//
//  *  For Text Sentiment:
//         CSV file(s) with each line in format:
//           ML_USE,TEXT_SNIPPET,SENTIMENT
//           TEXT_SNIPPET must have up to 500 characters.
//         Three sample rows:
//         TRAIN,"@freewrytin God is way too good for Claritin",2
//         TRAIN,"I need Claritin so bad",3
//         TEST,"Thank god for Claritin.",4
//
//  Definitions:
//  ML_USE = "TRAIN" | "VALIDATE" | "TEST" | "UNASSIGNED"
//           Describes how the given example (file) should be used for model
//           training. "UNASSIGNED" can be used when user has no preference.
//  GCS_FILE_PATH = A path to file on GCS, e.g. "gs://folder/image1.png".
//  LABEL = A display name of an object on an image, video etc., e.g. "dog".
//          Must be up to 32 characters long and can consist only of ASCII
//          Latin letters A-Z and a-z, underscores(_), and ASCII digits 0-9.
//          For each label an AnnotationSpec is created which display_name
//          becomes the label; AnnotationSpecs are given back in predictions.
//  INSTANCE_ID = A positive integer that identifies a specific instance of a
//                labeled entity on an example. Used e.g. to track two cars on
//                a video while being able to tell apart which one is which.
//  BOUNDING_BOX = VERTEX,VERTEX,VERTEX,VERTEX | VERTEX,,,VERTEX,,
//                 A rectangle parallel to the frame of the example (image,
//                 video). If 4 vertices are given they are connected by edges
//                 in the order provided, if 2 are given they are recognized
//                 as diagonally opposite vertices of the rectangle.
//  VERTEX = COORDINATE,COORDINATE
//           First coordinate is horizontal (x), the second is vertical (y).
//  COORDINATE = A float in 0 to 1 range, relative to total length of
//               image or video in given dimension. For fractions the
//               leading non-decimal 0 can be omitted (i.e. 0.3 = .3).
//               Point 0,0 is in top left.
//  TIME_SEGMENT_START = TIME_OFFSET
//                       Expresses a beginning, inclusive, of a time segment
//                       within an example that has a time dimension
//                       (e.g. video).
//  TIME_SEGMENT_END = TIME_OFFSET
//                     Expresses an end, exclusive, of a time segment within
//                     an example that has a time dimension (e.g. video).
//  TIME_OFFSET = A number of seconds as measured from the start of an
//                example (e.g. video). Fractions are allowed, up to a
//                microsecond precision. "inf" is allowed, and it means the end
//                of the example.
//  TEXT_SNIPPET = A content of a text snippet, UTF-8 encoded.
//  SENTIMENT = An integer between 0 and
//              Dataset.text_sentiment_dataset_metadata.sentiment_max
//              (inclusive). Describes the ordinal of the sentiment - higher
//              value means a more positive sentiment. All the values are
//              completely relative, i.e. neither 0 needs to mean a negative or
//              neutral sentiment nor sentiment_max needs to mean a positive one
//              - it is just required that 0 is the least positive sentiment
//              in the data, and sentiment_max is the  most positive one.
//              The SENTIMENT shouldn't be confused with "score" or "magnitude"
//              from the previous Natural Language Sentiment Analysis API.
//              All SENTIMENT values between 0 and sentiment_max must be
//              represented in the imported data. On prediction the same 0 to
//              sentiment_max range will be used. The difference between
//              neighboring sentiment values needs not to be uniform, e.g. 1 and
//              2 may be similar whereas the difference between 2 and 3 may be
//              huge.
//
//  Errors:
//  If any of the provided CSV files can't be parsed or if more than certain
//  percent of CSV rows cannot be processed then the operation fails and
//  nothing is imported. Regardless of overall success or failure the per-row
//  failures, up to a certain count cap, will be listed in
//  Operation.metadata.partial_failures.
//
message InputConfig {
  // Required. The source of the input.
  oneof source {
    // The GCS location for the input content.
    GcsSource gcs_source = 1;

    // The BigQuery location for the input content.
    BigQuerySource bigquery_source = 3;
  }
}

// Input configuration for BatchPredict Action.
//
// The format of input depends on the ML problem of the model used for
// prediction. As input source the
// [gcs_source][google.cloud.automl.v1beta1.InputConfig.gcs_source]
// is expected, unless specified otherwise.
//
// The formats are represented in EBNF with commas being literal and with
// non-terminal symbols defined near the end of this comment. The formats are:
//
//  *  For Video Classification:
//         CSV file(s) with each line in format:
//           GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END
//           GCS_FILE_PATH leads to video of up to 1.5GB in size and up to 1h
//           duration. Supported extensions: .MOV, .MPEG4, .MP4, .AVI.
//           TIME_SEGMENT_START and TIME_SEGMENT_END must be within the
//           length of the video, and end has to be after the start.
//         Three sample rows:
//           gs://folder/video1.mp4,10,40
//           gs://folder/video1.mp4,20,60
//           gs://folder/vid2.mov,0,inf
//
//  Definitions:
//  GCS_FILE_PATH = A path to file on GCS, e.g. "gs://folder/video.avi".
//  TIME_SEGMENT_START = TIME_OFFSET
//                       Expresses a beginning, inclusive, of a time segment
//                       within an
//                       example that has a time dimension (e.g. video).
//  TIME_SEGMENT_END = TIME_OFFSET
//                     Expresses an end, exclusive, of a time segment within
//                     an example that has a time dimension (e.g. video).
//  TIME_OFFSET = A number of seconds as measured from the start of an
//                example (e.g. video). Fractions are allowed, up to a
//                microsecond precision. "inf" is allowed and it means the end
//                of the example.
//
//  Errors:
//  If any of the provided CSV files can't be parsed or if more than certain
//  percent of CSV rows cannot be processed then the operation fails and
//  prediction does not happen. Regardless of overall success or failure the
//  per-row failures, up to a certain count cap, will be listed in
//  Operation.metadata.partial_failures.
//
message BatchPredictionInputConfig {
  // Required. The source of the input.
  oneof source {
    // The GCS location for the input content.
    GcsSource gcs_source = 1;

    // The BigQuery location for the input content.
    BigQuerySource bigquery_source = 2;
  }
}

// Output configuration.
message OutputConfig {
  // Required. The destination of the output.
  oneof destination {
    // The GCS location where the output must be written to.
    GcsDestination gcs_destination = 1;
  }
}

// Output configuration for BatchPredict Action.
//
// In provided destination directory a new directory will be created.
// Its name will be
// "prediction-<model-display-name>-<timestamp-of-prediction-call>",
// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. The contents
// of it depend on the ML problem the predictions are made for.
//
//  *  For Video Classification:
//         In the created directory a video_classification.csv file, and a .JSON
//         file per each video classification requested in the input (i.e. each
//         line in given CSV(s)), will be created.
//
//         The format of video_classification.csv is:
//
// GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END,JSON_FILE_NAME,STATUS
//         where:
//         GCS_FILE_PATH,TIME_SEGMENT_START,TIME_SEGMENT_END = matches 1 to 1
//             the prediction input lines (i.e. video_classification.csv has
//             precisely the same number of lines as the prediction input had.)
//         JSON_FILE_NAME = Name of .JSON file in the output directory, which
//             contains prediction responses for the video time segment.
//         STATUS = "OK" if prediction completed successfully, or an error
//             code and,or message otherwise. If STATUS is not "OK" then the
//             .JSON file for that line may not exist or be empty.
//
//         Each .JSON file, assuming STATUS is "OK", will contain a list of
//         AnnotationPayload protos in JSON format, which are the predictions
//         for the video time segment the file is assigned to in the
//         video_classification.csv. All AnnotationPayload protos will have
//         video_classification field set, and will be sorted by
//         video_classification.type field (note that the returned types are
//         governed by `classifaction_types` parameter in
//         [PredictService.BatchPredictRequest.params][]).
message BatchPredictionOutputConfig {
  // Required. The destination of the output.
  oneof destination {
    // The GCS location of the directory where the output must be written to.
    GcsDestination gcs_destination = 1;
  }
}

// Output configuration for ModelExport Action.
//
// The given back output depends on the type of the model.
//
// *  For Image Classification mobile-low-latency-1, mobile-versatile-1,
//        mobile-high-accuracy-1:
//
//        Under the directory given as the destination a new one with name
//        "model-export-<model-display-name>-<timestamp-of-export-call>",
//        where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format, will
//        be created. Inside the model and any of its supporting files will be
//        written, as described
//
// [here](https:
// //cloud.google.com/vision/automl/alpha/docs/predict#deployment_to_devices).
message ModelExportOutputConfig {
  // Required. The destination of the output.
  oneof destination {
    // The GCS location where the output must be written to.
    GcsDestination gcs_destination = 1;
  }

  // Additional model-type-specific parameters describing the requirements for
  // the to be exported model files, any string must be up to 25000 characters
  // long.
  //
  // *  For Image Classification mobile-low-latency-1, mobile-versatile-1,
  //        mobile-high-accuracy-1:
  //
  //      `tf-version` - (string) Tensorflow/TFLite version the model should be
  //        exported in. Currently the only available value is "TF_1_9", which
  //        is also the default.
  //      `model-format` - (string) The format in which the model should be
  //        exported. Available values are "tflite" and "edgetpu_tflite" with
  //        former being the default.
  map<string, string> params = 2;
}

// The GCS location for the input content.
message GcsSource {
  // Required. Google Cloud Storage URIs to input files, up to 2000 characters
  // long. Accepted forms:
  // * Full object path, e.g. gs://bucket/directory/object.csv
  repeated string input_uris = 1;
}

// The BigQuery location for the input content.
message BigQuerySource {
  // Required. BigQuery URI to a table, up to 2000 characters long.
  // Accepted forms:
  // *  BigQuery gs path e.g. bq://projectId.bqDatasetId.bqTableId
  string input_uri = 1;
}

// The GCS location where the output must be written to
message GcsDestination {
  // Required. Google Cloud Storage URI to output directory, up to 2000
  // characters long.
  // Accepted forms:
  // * Prefix path: gs://bucket/directory
  // The requesting user must have write permission to the bucket.
  // The directory is created if it doesn't exist.
  string output_uri_prefix = 1;
}
